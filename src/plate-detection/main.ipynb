{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecter les plaques d'immatriculation des vehicules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "from customDataset import CustomDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"runnning on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folders = [r\"..\\..\\data\\images\\cars\\fr\", r\"..\\..\\data\\images\\cars\\de\", r\"..\\..\\data\\images\\cars\\pl\"]\n",
    "batch_size = 16\n",
    "image_size = 416\n",
    "dataset = CustomDataset(image_folders, image_size=image_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "len(dataloader), len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir le DataLoader pour obtenir les 6 premiers éléments\n",
    "num_images = 6\n",
    "images_to_display = []\n",
    "annotations_to_display = []\n",
    "for images, annotations in dataloader:\n",
    "    for image, annotation in zip(images, annotations):\n",
    "        images_to_display.append(image)\n",
    "        annotations_to_display.append(annotation)\n",
    "        if len(images_to_display) >= num_images:\n",
    "            break\n",
    "    if len(images_to_display) >= num_images:\n",
    "        break\n",
    "# Affichage des images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image = TF.to_pil_image(images_to_display[i])\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Image {i+1}\")\n",
    "    # Dessiner les bounding boxes\n",
    "    annotation = annotations_to_display[i]\n",
    "    x_min, y_min, x_max, y_max = annotation.tolist()\n",
    "    rect = plt.Rectangle(\n",
    "        (x_min, y_min), x_max - x_min, y_max - y_min, edgecolor=\"r\", facecolor=\"none\"\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class YOLO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YOLO, self).__init__()\n",
    "        # Couches de convolution pour extraire les caractéristiques de l'image\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        # Couches entièrement connectées pour prédire les bounding boxes\n",
    "        self.fc1 = nn.Linear(256 * 13 * 13, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)  # 4 pour les coordonnées\n",
    "\n",
    "        # Initialisation des poids\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passer l'image à travers les couches de convolution\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        # # Aplatir les caractéristiques pour les passer à travers les couches entièrement connectées\n",
    "        x = x.view(-1, 256 * 13 * 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # # Redimensionner les sorties pour obtenir les prédictions de bounding boxes\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "\n",
    "# net = YOLO()\n",
    "# input_data = torch.randn(13, 1, image_size, image_size)\n",
    "# output_data = net(input_data)\n",
    "# print(\"La taille de sortie :\", output_data.size())\n",
    "# Instanciation du modèle\n",
    "model = YOLO().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Boucle d'entraînement\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "n_batch = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1}...\")\n",
    "    for batch in tqdm(dataloader, total=n_batch):\n",
    "        images = batch[0].to(device)\n",
    "        annotations = batch[1].to(device)\n",
    "        # Réinitialiser les gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Propagation avant\n",
    "        predictions = model(images)\n",
    "        # Calcul de la perte\n",
    "        loss = criterion(annotations, predictions)\n",
    "        # Rétropropagation\n",
    "        loss.backward()\n",
    "        # Mise à jour des poids\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / n_batch\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for plotting\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot training losses as a blue line\n",
    "plt.plot(train_losses, color=\"blue\", marker=\"o\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Add legend and labels\n",
    "plt.legend([\"Train Loss\"], loc=\"upper right\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_annotations = next(iter(dataloader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(batch_images)\n",
    "print(\"MSE :\", criterion(predictions, batch_annotations).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des images\n",
    "n=2\n",
    "fig, axes = plt.subplots(n, len(batch_images)//n, figsize=(12, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i == len(batch_images) :\n",
    "        break\n",
    "    image = TF.to_pil_image(batch_images[i])\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis(\"off\")\n",
    "    # Dessiner les bounding boxes\n",
    "    annotation = batch_annotations[i]\n",
    "    annotation_pred = predictions[i]\n",
    "    x_min_pred, y_min_pred, x_max_pred, y_max_pred = annotation_pred[:4].tolist()\n",
    "    x_min, y_min, x_max, y_max = annotation.tolist()\n",
    "    rect = plt.Rectangle(\n",
    "        (x_min, y_min), x_max - x_min, y_max - y_min, edgecolor=\"g\", facecolor=\"none\"\n",
    "    )\n",
    "    rect_pred = plt.Rectangle(\n",
    "        (x_min_pred, y_min_pred), x_max_pred - x_min_pred, y_max_pred - y_min_pred, edgecolor=\"r\", facecolor=\"none\"\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.add_patch(rect_pred)\n",
    "    c = criterion(annotation_pred.unsqueeze(0), annotation.unsqueeze(0))\n",
    "    ax.set_title(f\"MSE={c.item():.2f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
